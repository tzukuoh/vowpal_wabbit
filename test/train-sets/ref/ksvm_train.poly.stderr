using l2 regularization = 1
predictions = ksvm_train.poly.predict
Lambda = 1
Kernel = poly
degree = 2
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/rcv1_smaller.dat
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0   1.0000   0.0000       50
1.236589 1.473179            2            2.0  -1.0000   0.4732      103
1.070077 0.903564            4            4.0  -1.0000  -0.2366      134
0.975140 0.880204            8            8.0  -1.0000  -0.3866      145
0.934707 0.894274           16           16.0   1.0000  -0.3350       23
0.925318 0.915930           32           32.0  -1.0000  -0.2578       31
0.907599 0.889880           64           64.0  -1.0000  -0.6310       60
0.910526 0.913452          128          128.0   1.0000   0.2649      105

finished run
number of examples = 250
weighted example sum = 250.000000
weighted label sum = -22.000000
average loss = 0.815429
best constant = -0.088000
best constant's loss = 0.992256
total feature number = 19870
Num support = 247
Number of kernel evaluations = 170641 Number of cache queries = 119344
Total loss = 203.857254
Done freeing model
Done freeing kernel params
Done with finish 
